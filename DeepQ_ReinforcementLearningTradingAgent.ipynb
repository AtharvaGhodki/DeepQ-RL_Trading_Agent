{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"83708667-4fdc-1563-7b3a-06b6575d2865","id":"TKjzidcZwtf0"},"source":["#Deep Q Reinforcement Learning based Trading Strategy\n","\n","In this code, we will create an end-to-end trading strategy based on Reinforcement Learning.\n"]},{"cell_type":"markdown","metadata":{"id":"g7oy9IWuwtf2"},"source":["<a id='0'></a>\n","# 1. Problem Definition"]},{"cell_type":"markdown","metadata":{"id":"zfji5LeVwtf3"},"source":["In this Reinforcement Learning framework for trading strategy, the algorithm takes an action (buy, sell or hold) depending upon the current state of the stock price. The algorithm is trained using Deep Q-Learning framework, to help us predict\n","the best action, based on the current stock prices.\n","\n","The key components of the RL based framework are :\n","* Agent: Trading agent.\n","* Action: Buy, sell or hold.\n","* Reward function: Realized profit and loss (PnL) is used as the reward function\n","for this case study. The reward depends upon the action:\n","    * Sell: Realized profit and loss (sell price - bought price)\n","    * Buy: No reward\n","    * Hold: No Reward\n","\n","* State: Differences of past stock prices for a given time window is used as the state.\n","\n","The data used for this case study will be the NIFTY's 2011-2019."]},{"cell_type":"markdown","metadata":{"id":"OMONqYHEwtf3"},"source":["<a id='1'></a>\n","# 2. Getting Started- Loading the data and python packages"]},{"cell_type":"markdown","metadata":{"id":"4Ua63dh_wtf4"},"source":["<a id='1.1'></a>\n","## 2.1. Loading the python packages"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d8fee34-f454-2642-8b06-ed719f0317e1","executionInfo":{"elapsed":5116,"status":"ok","timestamp":1723715346130,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"gXIim2KZwtf4"},"outputs":[],"source":["# Load libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pandas import read_csv, set_option\n","from pandas.plotting import scatter_matrix\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","import datetime\n","import math\n","from numpy.random import choice\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","#Import Model Packages for reinforcement learning\n","from keras import layers, models, optimizers\n","from keras import backend as K\n","from collections import namedtuple, deque"]},{"cell_type":"markdown","metadata":{"id":"IL10UiSVwtf5"},"source":["<a id='1.2'></a>\n","## 2.2. Loading the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":472,"status":"ok","timestamp":1723715349652,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"4sKvIxmAwtf5"},"outputs":[],"source":["dataset = read_csv('2011-2019pricedata.csv',index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":451,"status":"ok","timestamp":1723715352313,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"codk3vVHwtf5"},"outputs":[],"source":["#Diable the warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"elapsed":1034,"status":"ok","timestamp":1723715354864,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"1NKOSP0Fwtf5","outputId":"3c231b48-f113-4c11-fd82-244c9f9cc449"},"outputs":[],"source":["type(dataset)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"df6a4523-b385-69ee-c933-592826d81431","id":"0pJZiP6uwtf6"},"source":["<a id='2'></a>\n","# 3. Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52f85dc2-0f91-3c50-400e-ddc38bea966b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1723715359968,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"gj3DjORNwtf6","outputId":"ebe5f029-6643-48a2-94e1-37645fdd9c18"},"outputs":[],"source":["# shape\n","dataset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1723715364741,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"l4FPzeU-wtf6","outputId":"32dc043e-abe1-4977-ff81-dcea922a0e55"},"outputs":[],"source":["dataset.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bffeec0-5bbc-fffb-18f2-3da56b862ca3","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":593,"status":"ok","timestamp":1723715370462,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"m9dimK7Jwtf6","outputId":"ee876fc8-452d-47b7-e28b-68460c3157f5"},"outputs":[],"source":["dataset.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":779,"status":"ok","timestamp":1723715381956,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"GhFMWmFKwtf6","outputId":"322a55b5-5c50-498e-edc0-535bd5970136"},"outputs":[],"source":["dataset['Close'].plot()"]},{"cell_type":"markdown","metadata":{"id":"hrsWjUoRwtf7"},"source":["<a id='3'></a>\n","## 4. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"-3t1m84lwtf7"},"source":["<a id='3.1'></a>\n","## 4.1. Data Cleaning\n","Let us check for the NAs in the rows, either drop them or fill them with the mean of the column"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":458,"status":"ok","timestamp":1723715394489,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"easmQPeTwtf7","outputId":"ce88acdf-fee1-49e8-f0fe-1a607eb06d6a"},"outputs":[],"source":["#Checking for any null values and removing the null values'''\n","print('Null Values =',dataset.isnull().values.any())"]},{"cell_type":"markdown","metadata":{"id":"fljSj7Cswtf7"},"source":["In case there are null values fill the missing values with the last value available in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1723715407501,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"t3RSPz-Uwtf7","outputId":"c01e3ad3-b2d0-40fc-c6e4-613690d5eb54"},"outputs":[],"source":["# Fill the missing values with the last value available in the dataset.\n","dataset=dataset.fillna(method='ffill')\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"Y_jWC7j0wtf7"},"source":["<a id='4'></a>\n","# 5. Evaluate Algorithms and Models"]},{"cell_type":"markdown","metadata":{"id":"bHa6TTKmwtf7"},"source":["<a id='5.1'></a>\n","## 5.1. Train Test Split"]},{"cell_type":"markdown","metadata":{"id":"s1Ohkjqpwtf7"},"source":["We will use 80% of the dataset for modeling\n","and use 20% for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":713,"status":"ok","timestamp":1723715421199,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"jydx2_Tgwtf7"},"outputs":[],"source":["X=list(dataset[\"Close\"])\n","X=[float(x) for x in X]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":455,"status":"ok","timestamp":1723715430440,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"_nBQaPLtwtf7"},"outputs":[],"source":["validation_size = 0.2\n","#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n","#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n","train_size = int(len(X) * (1-validation_size))\n","X_train, X_test = X[0:train_size], X[train_size:len(X)]"]},{"cell_type":"markdown","metadata":{"id":"GbElLlm4wtf7"},"source":["<a id='5.2'></a>\n","## 5.2. Implementation steps and modules"]},{"cell_type":"markdown","metadata":{"id":"jqE8tC8Pwtf7"},"source":["The algorithm, in simple terms decides whether to buy, sell or hold, when provided\n","with the current market price. The algorithm is based on “Q-learning based”\n","approach and used Deep-Q-Network (DQN) to come up with a policy. As discussed\n","before, the name “Q-learning” comes from the Q(s, a) function, that based on the\n","state s and provided action a returns the expected reward.\n","\n","\n","In order to implement this DQN algorithm several functions and modules are implemented that interact with each other during the model training. A summary of the\n","modules and functions is described below.\n","\n","1. **Agent Class**: The agent is defined as “Agent” class, that holds the variables and\n","member functions that perform the Q-Learning that we discussed before. An\n","object of the “Agent” class is created using the training phase and is used for\n","training the model.\n","2. **Helper functions**: In this module, we create additional functions that are helpful\n","for training. There are two helper functions that we have are as follows.\n","3. **Training module**: In this step, we perform the training of the data using the vari‐\n","ables and the functions agent and helper methods. This will provide us with one\n","of three actions (i.e. buy, sell or hold) based on the states of the stock prices at the\n","end of the day. During training, the prescribed action for each day is predicted,\n","the rewards are computed and the deep-learning based Q-learning model\n","weights are updated iteratively over a number of episodes."]},{"cell_type":"markdown","metadata":{"id":"6K9zYwdIwtf7"},"source":["<a id='5.3'></a>\n","## 5.3. Agent script"]},{"cell_type":"markdown","metadata":{"id":"_6fvyWr_wtf7"},"source":["The definition of the Agent script is the key step, as it consists of the In this section, we will train an agent that will perform reinforcement learning based on the Q-Learning. We will perform the following steps to achieve this:\n","\n","* Create an agent class whose initial function takes in the batch size, state size, and an evaluation Boolean function, to check whether the training is ongoing.\n","* In the agent class, create the following methods:\n","    * Constructor: The constructor inititalises all the parameters.\n","    * Model : This f unction has a deep learning model to map the state to action.\n","    * Act function :Returns an action, given a state, using the  output of the model function. The number of actions are defined as 3: hold, buy, sell\n","    * expReplay : Create a Replay function that adds, samples, and evaluates a buffer. Add a new experience to the replay buffer memory. Randomly sample a batch of experienced tuples from the memory. In the following function, we randomly sample states from a memory buffer. Experience replay stores a history of state, action, reward, and next state transitions that are experienced by the agent. It randomly samples mini-batches from this experience to update the network weights at each time step before the agent selects an ε-greedy action.\n","\n","Experience replay increases sample efficiency, reduces the autocorrelation of samples that are collected during online learning, and limits the feedback due to the current weights producing training samples that can lead to local minima or divergence."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1723715809461,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"EvWXiWvJwtf8"},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from IPython.core.debugger import set_trace\n","\n","import numpy as np\n","import random\n","from collections import deque\n","\n","class Agent:\n","    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n","        #State size depends and is equal to the the window size, n previous days\n","        self.state_size = state_size # normalized previous days,\n","        self.action_size = 3 # sit, buy, sell\n","        self.memory = deque(maxlen=1000)\n","        self.inventory = []\n","        self.model_name = model_name\n","        self.is_eval = is_eval\n","\n","        self.gamma = 0.95\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.85\n","        #self.epsilon_decay = 0.9\n","\n","\n","        self.model = load_model(model_name) if is_eval else self._model()\n","\n","    #Deep Q Learning model- returns the q-value when given state as input\n","    def _model(self):\n","        model = Sequential()\n","        #Input Layer\n","        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n","        #Hidden Layers\n","        model.add(Dense(units=32, activation=\"relu\"))\n","        model.add(Dense(units=8, activation=\"relu\"))\n","        #Output Layer\n","        model.add(Dense(self.action_size, activation=\"linear\"))\n","        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n","        return model\n","\n","    def act(self, state):\n","        #If it is test and self.epsilon is still very high, once the epsilon become low, there are no random\n","        #actions suggested.\n","        if not self.is_eval and random.random() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        options = self.model.predict(state)\n","        #set_trace()\n","        #action is based on the action that has the highest value from the q-value function.\n","        return np.argmax(options[0])\n","\n","    def expReplay(self, batch_size):\n","        mini_batch = []\n","        l = len(self.memory)\n","        for i in range(l - batch_size + 1, l):\n","            mini_batch.append(self.memory[i])\n","\n","        # the memory during the training phase.\n","        for state, action, reward, next_state, done in mini_batch:\n","            target = reward\n","            if not done:\n","                #max of the array of the predicted.\n","                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n","\n","            target_f = self.model.predict(state)\n","            target_f[0][action] = target\n","            #train and fit the model where state is X and target_f is Y, where the target is updated.\n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"markdown","metadata":{"id":"3Ag0XGlrwtf8"},"source":["<a id='5.4'></a>\n","## 5.4. Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"RVXMR9ZJwtf8"},"source":["In this script, we will create functions that will be helpful for training. We create the following functions:\n","\n","1) formatPrice:format the price to two decimal places, to reduce the ambiguity of the data:\n","\n","2) getState: Define a function to generate states from the input vector. Create the time series by generating the states from the vectors created in the previous step. The function for this takes three parameters: the data; a time, t (the day that you want to predict); and a window (how many days to go back in time). The rate of change between these vectors will then be measured and based on the sigmoid function."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":439,"status":"ok","timestamp":1723715812771,"user":{"displayName":"Atharva Sham Ghodki","userId":"12107905115569029453"},"user_tz":-330},"id":"2N2Kdvrhwtf8"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","# prints formatted price\n","def formatPrice(n):\n","    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n","\n","# returns the sigmoid\n","def sigmoid(x):\n","    return 1 / (1 + math.exp(-x))\n","\n","# returns an an n-day state representation ending at time t\n","\n","def getState(data, t, n):\n","    d = t - n + 1\n","    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n","    #block is which is the for [1283.27002, 1283.27002]\n","    res = []\n","    for i in range(n - 1):\n","        res.append(sigmoid(block[i + 1] - block[i]))\n","    return np.array([res])"]},{"cell_type":"markdown","metadata":{"id":"rIUFkod8wtf8"},"source":["<a id='5.5'></a>\n","## 5.5. Training the data"]},{"cell_type":"markdown","metadata":{"id":"gGnnwjlOwtf9"},"source":["We will proceed to train the data, based on our agent and helper methods. This will provide us with one of three actions, based on the states of the stock prices at the end of the day. These states can be to buy, sell, or hold. During training, the prescribed action for each day is predicted, and the price (profit, loss, or unchanged) of the action is calculated. The cumulative sum will be calculated at the end of the training period, and we will see whether there has been a profit or a loss. The aim is to maximize the total profit.\n","\n","Steps:\n","* Define the number of market days to consider as the window size and define the batch size with which the neural network will be trained.\n","* Instantiate the stock agent with the window size and batch size.\n","* The episode count is defined. The agent will look at the data for so many numbers of times. An episode represents a complete pass over the data.\n","* We can start to iterate through the episodes.\n","* Each episode has to be started with a state based on the data and window size. The inventory of stocks is initialized before going through the data.\n","* **Start to iterate over every day of the stock data. The action probability is predicted by the agent**.\n","* Next, every day of trading is iterated, and the agent can act upon the data. Every day, the agent decides an action. Based on the action, the stock is held, sold, or bought.\n","* If the action is 1, then agent buys the stock.\n","* If the action is 2, the agent sells the stocks and removes it from the inventory. Based on the sale, the profit (or loss) is calculated.\n","\n","* If the action is 0, then there is no trade. The state can be called holding during that period.\n","* The details of the state, next state, action etc is saved in the memory of the agent object, which is used further by the exeReply function.       \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XvFLlelwtf9","outputId":"80bf13f2-9e16-4dc8-c4cb-5e663c263839"},"outputs":[],"source":["from IPython.core.debugger import set_trace\n","window_size = 5\n","agent = Agent(window_size)\n","#In this step we feed the closing value of the stock price\n","data = X_train\n","l = len(data) - 1\n","#\n","batch_size = 32\n","#An episode represents a complete pass over the data.\n","episode_count = 1\n","\n","for e in range(episode_count + 1):\n","    print(\"Running episode \" + str(e) + \"/\" + str(episode_count))\n","    state = getState(data, 0, window_size + 1)\n","    total_profit = 0\n","    agent.inventory = []\n","    states_sell = []\n","    states_buy = []\n","    for t in range(l):\n","        action = agent.act(state)\n","        # sit\n","        next_state = getState(data, t + 1, window_size + 1)\n","        reward = 0\n","\n","        if action == 1: # buy\n","            agent.inventory.append(data[t])\n","            states_buy.append(t)\n","            #print(\"Buy: \" + formatPrice(data[t]))\n","\n","        elif action == 2 and len(agent.inventory) > 0: # sell\n","            bought_price = agent.inventory.pop(0)\n","            reward = max(data[t] - bought_price, 0)\n","            total_profit += data[t] - bought_price\n","            states_sell.append(t)\n","            #print(\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n","\n","        done = True if t == l - 1 else False\n","        #appends the details of the state action etc in the memory, which is used further by the exeReply function\n","        agent.memory.append((state, action, reward, next_state, done))\n","        state = next_state\n","\n","        if done:\n","            print(\"--------------------------------\")\n","            print(\"Total Profit: \" + formatPrice(total_profit))\n","            print(\"--------------------------------\")\n","        if len(agent.memory) > batch_size:\n","            agent.expReplay(batch_size)\n","\n","\n","    if e % 2 == 0:\n","        agent.model.save(\"model_ep\" + str(e))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwcEuV_5wtf9","outputId":"ccc3cd5a-4b55-43b1-f04a-192e7ac5a81a"},"outputs":[],"source":["#Deep Q-Learning Model\n","print(agent.model.summary())"]},{"cell_type":"markdown","metadata":{"id":"EfOWbqyOwtf9"},"source":["<a id='6'></a>\n","# 6. Testing the Data"]},{"cell_type":"markdown","metadata":{"id":"Hq-6Pt1cwtf9"},"source":["After training the data, it is tested it against the test dataset. Our model resulted in a overall profit. The best thing about the model was that the profits kept improving over time, indicating that it was learning well and taking better actions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LU09Jgpwtf9"},"outputs":[],"source":["#agent is already defined in the training set above.\n","test_data = X_test\n","l_test = len(test_data) - 1\n","state = getState(test_data, 0, window_size + 1)\n","total_profit = 0\n","is_eval = True\n","done = False\n","states_sell_test = []\n","states_buy_test = []\n","#Get the trained model\n","model_name = \"model_ep\"+str(episode_count)\n","agent = Agent(window_size, is_eval, model_name)\n","state = getState(data, 0, window_size + 1)\n","total_profit = 0\n","agent.inventory = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugbYY5hEwtf9","outputId":"9eb9e65c-2630-4f53-906c-e25cf432286d"},"outputs":[],"source":["for t in range(l_test):\n","    action = agent.act(state)\n","    #print(action)\n","    #set_trace()\n","    next_state = getState(test_data, t + 1, window_size + 1)\n","    reward = 0\n","\n","    if action == 1:\n","        agent.inventory.append(test_data[t])\n","        states_buy_test.append(t)\n","        print(\"Buy: \" + formatPrice(test_data[t]))\n","\n","    elif action == 2 and len(agent.inventory) > 0:\n","        bought_price = agent.inventory.pop(0)\n","        reward = max(test_data[t] - bought_price, 0)\n","        #reward = test_data[t] - bought_price\n","        total_profit += test_data[t] - bought_price\n","        states_sell_test.append(t)\n","        print(\"Sell: \" + formatPrice(test_data[t]) + \" | profit: \" + formatPrice(test_data[t] - bought_price))\n","\n","    if t == l_test - 1:\n","        done = True\n","    agent.memory.append((state, action, reward, next_state, done))\n","    state = next_state\n","\n","    if done:\n","        print(\"------------------------------------------\")\n","        print(\"Total Profit: \" + formatPrice(total_profit))\n","        print(\"------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"lIeTtydCwtf9"},"source":["Looking at the results above, our model resulted in an overall profit of $1280, and we can say that our DQN agent performs quite well on the test set. However, the performance of the model can be further improved by optimizing the hyperparameters as discussed in the model tuning section before. Also, given high complexity and low interpretability of the model, ideally there should be more tests conducted on different time periods before deploying the model for live trading."]},{"cell_type":"markdown","metadata":{"id":"Fn3hxrUtwtf-"},"source":["**Conclusion**\n","\n","We observed that we don’t have to decide the strategy or policy\n","for trading. The algorithm decides the policy by itself, and the overall approach is\n","much simpler and more principled than the supervised learning-based approach.\n","\n","The\n","policy can be parameterized by a complex model, such as a deep neural network, and\n","we can learn policies that are more complex and powerful than any rules a human\n","trader.\n","\n","We used the testing set to evaluate the model and found an overall profit in the test set."]}],"metadata":{"_change_revision":206,"_is_fork":false,"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
